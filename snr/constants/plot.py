TASK_CATEGORIES = {
    'hellaswag': 'language',
    'winogrande': 'language',
    'arc_challenge': 'knowledge',
    'arc_easy': 'knowledge', 
    'boolq': 'knowledge',
    'csqa': 'knowledge',
    'openbookqa': 'knowledge',
    'piqa': 'knowledge',
    'socialiqa': 'knowledge',
    'drop': 'knowledge',
    'jeopardy': 'knowledge',
    'squad': 'knowledge', 
    'triviaqa': 'knowledge',
    'olmes_core9': 'knowledge',
    'mmlu': 'knowledge',
    'olmes_core9_mc': 'knowledge',
    'mmlu_mc': 'knowledge',
    'olmes_gen': 'knowledge',
    'autobencher': 'knowledge',
    'autobencher:mc': 'knowledge',
    'mmlu_pro': 'knowledge',
    'agi_eval': 'knowledge',
    'medmcqa': 'knowledge',
    'gsm8k': 'math',
    'minerva': 'math',
    'minerva_math_algebra': 'math',
    'minerva_math_counting_and_probability': 'math',
    'minerva_math_geometry': 'math',
    'minerva_math_intermediate_algebra': 'math',
    'minerva_math_number_theory': 'math',
    'minerva_math_prealgebra': 'math',
    'minerva_math_precalculus': 'math',
    'gsm_plus': 'math',
    'gsm_symbolic_main': 'math',
    'gsm_symbolic_p1': 'math',
    'minerva_math_500': 'math',
    'gsm_symbolic_p2': 'math',
    'mbpp': 'code',
    'mbppplus': 'code',
    'codex_humaneval': 'code',
    'codex_humanevalplus': 'code',
    'bbh': 'reasoning',
    'paloma_c4_en': 'loss',
    'paloma_m2d2_s2orc_unsplit': 'loss',
}
CATEGORIES = set(TASK_CATEGORIES.values())

CATEGORY_COLORS = {
    'language': '#2ecc71',
    'knowledge': '#3498db', 
    'math': '#e74c3c',
    'code': '#9b59b6',
    'loss': '#f1c40f',
}
CATEGORY_COLORS_SMALL = {cat: color for cat, color in CATEGORY_COLORS.items() if ':' not in cat}

SIZE_COLORS = {
    '4M': 'brown',
    '6M': '#7f7f7f', 
    '8M': '#17becf', 
    '10M': '#bcbd22',
    '14M': '#e377c2',
    '16M': '#8c564b',
    '20M': 'black',
    '60M': 'teal',
    '90M': 'pink',
    '150M': '#1f77b4',
    '300M': '#2ca02c',
    '530M': '#ff7f0e',
    '750M': '#d62728',
    '1B': '#9467bd'
}

MODEL_FAMILY_COLORS = {
    'codeqwen': '#2ca02c',
    'codestral': '#ff7f0e',
    'deepseek': '#1f77b4',
    'gemma': '#9467bd',
    'llama': '#d62728',
    'mathstral': '#e377c2',
    'meta': '#7f7f7f',
    'mistral': '#bcbd22',
    'mixtral': '#17becf',
    'olmo': '#8c564b',
    'orca': '#e7ba52',
    'phi': '#393b79',
    'qwen': '#637939',
    'smollm': '#b5cf6b',
    'stablelm': '#843c39',
    'yi': '#9c9ede',

    # by hf family
    'allenai': '#f0529c',
    'meta-llama': '#7f7f7f',
    'microsoft': '#e7ba52',
    'Qwen': '#637939',
    'HuggingFaceTB': '#b5cf6b',
    'deepseek-ai': '#1f77b4',
    '01-ai': '#9c9ede',
    'huggyllama': '#393b79',
    'stabilityai': '#843c39',
    'google': '#9467bd',
    'mistral': '#bcbd22',
}

EXTERNAL_SCALING_COLOR_MAP = {
    'Qwen': 'green',
    'Llama': 'blue',
    'LLaMA': 'blue',
    'Mistral': 'orange',
    '3B': 'black',
    'OLMo': 'pink',
    'pythia': 'brown',
    'gemma': 'teal',
    'phi': 'black',
    'deepseek': 'pink',
    'zephyr': 'green',
    'neo': 'orange',
    'falcon': 'blue',
    'starcoder': 'grey',
    'stablelm': 'grey',
}

SHORT_TASK_NAME = {
    'jeopardy': 'olmes_all',
    'boolq': 'multitask_all',
    'gsm_plus': 'multitask_math',
    'mbpp': 'multitask_code',
    'medmcqa': 'multitask_knowledge',
    'mmlu_pro_': 'mmlu_pro',
    'mmlu_abstract_algebra:mc': 'mmlu_mc',
    'mmlu': 'mmlu',
    'minerva': 'minerva',
    'agi_eval': 'agi_eval',
    'bbh': 'bbh',
    'arc_challenge:para': 'olmes_core9_para',
    'arc_challenge:distractors': 'olmes_core9_distractors',
    'arc_challenge:enlarge': 'olmes_core9_enlarge',
    'arc_challenge:mc': 'olmes_core9_mc',
    'arc_challenge': 'olmes_core9',
    'drop': 'olmes_gen',
}

PRETTY_TASK_NAMES = {
    'arc_challenge:mc': 'ARC Challenge MC',
    'arc_challenge': 'ARC Challenge',
    'arc_easy:mc': 'ARC Easy MC', 
    'arc_easy': 'ARC Easy', 
    'autobencher:mc': 'Autobencher MC',
    'autobencher': 'AutoBencher',
    'boolq:mc': 'BoolQ MC',
    'boolq': 'BoolQ',
    'codex_humaneval': 'HumanEval',
    'codex_humanevalplus': 'HumanEval+',
    'csqa:mc': 'CommonsenseQA MC',
    'csqa': 'CommonsenseQA',
    'drop': 'DROP',
    'gsm8k': 'GSM8K',
    'hellaswag:mc': 'HellaSwag MC',
    'hellaswag': 'HellaSwag',
    'jeopardy': 'Jeopardy',
    'mbpp': 'MBPP',
    'mbppplus': 'MBPP+',
    'minerva': 'Minerva MATH',
    'mmlu_mc': 'MMLU MC',
    'mmlu': 'MMLU',
    'olmes_core9_mc': 'OLMES Core 9 MC',
    'olmes_core9': 'OLMES Core 9',
    'olmes_gen': 'OLMES Gen',
    'openbookqa:mc': 'OpenBookQA MC',
    'openbookqa': 'OpenBookQA',
    'paloma_c4_en': 'Paloma C4',
    'paloma_m2d2_s2orc_unsplit': 'Paloma M2D2',
    'piqa:mc': 'PIQA MC',
    'piqa': 'PIQA',
    'socialiqa:mc': 'SocialIQA MC',
    'socialiqa': 'SocialIQA', 
    'squad': 'SQuAD',
    'triviaqa': 'TriviaQA',
    'winogrande:mc': 'WinoGrande MC',
    'winogrande': 'WinoGrande',
    'agi_eval': 'AGI Eval',
    'aime': 'AIME',
    'bbh': 'BBH',
    'gsm_plus': 'GSM+',
    'gsm_symbolic_main': 'GSM Symbolic',
    'gsm_symbolic_p1': 'GSM Symbolic P1',
    'gsm_symbolic_p2': 'GSM Symbolic P2', 
    'medmcqa': 'MedMCQA',
    'minerva_math_500': 'Minerva MATH 500',
    'mmlu_pro': 'MMLU Pro',
    'olmes_10_macro_avg': 'OLMES 10 Avg.',
    'multitask_math': 'Math Tasks',
    'multitask_code': 'Code Tasks',
    'multitask_knowledge': 'Knowledge Tasks',
    'olmes_all': 'OLMES + Gen',
    'multitask_all': 'All Tasks',
}